{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Aula: Introdução às Redes Neurais\n",
        "\n"
      ],
      "metadata": {
        "id": "Gw6hVVuCmjm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 1. O que são Redes Neurais?\n",
        "\n",
        "Redes neurais artificiais são modelos computacionais inspirados no cérebro humano. Elas são utilizadas para reconhecer padrões e resolver problemas complexos. O conceito central de uma rede neural é o **neurônio artificial**, que recebe entradas, aplica pesos e uma função de ativação, e gera uma saída.\n",
        "\n",
        "Uma rede neural pode ser representada matematicamente por:\n",
        "\n",
        "$$ y = f \\left( \\sum_{i=1}^{n} w_i x_i + b \\right) $$\n",
        "\n",
        "onde:\n",
        "- $ x_i $ são as entradas,\n",
        "- $ w_i $ são os pesos associados a cada entrada,\n",
        "- $ b $ é o viés (bias),\n",
        "- $ f $ é a função de ativação,\n",
        "- $ y $ é a saída do neurônio.\n",
        "\n",
        "Ajustando os pesos e o viés, conseguimos treinar a rede para realizar tarefas específicas.\n"
      ],
      "metadata": {
        "id": "GzHOpIShmm67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. Arquitetura das Redes Neurais\n",
        "\n",
        "Uma rede neural é composta por várias **camadas** de neurônios:\n",
        "\n",
        "- **Camada de entrada**: recebe os dados de entrada.\n",
        "- **Camadas ocultas**: aplicam transformações não lineares aos dados.\n",
        "- **Camada de saída**: fornece a previsão do modelo.\n",
        "\n",
        "O número de neurônios em cada camada e o tipo de função de ativação influenciam diretamente o desempenho da rede.\n",
        "\n",
        "A equação geral de uma rede com múltiplas camadas é:\n",
        "\n",
        "$$ a^{(l)} = f \\left( W^{(l)} a^{(l-1)} + b^{(l)} \\right) $$\n",
        "\n",
        "onde:\n",
        "- $ a^{(l)} $ é a ativação da camada $ l $,\n",
        "- $ W^{(l)} $ é a matriz de pesos da camada $ l $,\n",
        "- $ b^{(l)} $ é o vetor de viés da camada $ l $,\n",
        "- $ f $ é a função de ativação.\n"
      ],
      "metadata": {
        "id": "Gl7hRjatmpj1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 3. Treinamento de uma Rede Neural\n",
        "\n",
        "O treinamento de uma rede neural envolve três passos principais:\n",
        "\n",
        "1. **Forward Propagation** (propagação para frente): os dados passam pelas camadas e a saída é calculada.\n",
        "2. **Cálculo da Função de Custo**: mede a diferença entre a previsão da rede e o valor real esperado.\n",
        "3. **Backpropagation** (propagação para trás): ajusta os pesos da rede para minimizar o erro.\n",
        "\n",
        "A atualização dos pesos é feita usando o **Gradiente Descendente**, um algoritmo que ajusta os pesos iterativamente para minimizar a função de custo:\n",
        "\n",
        "$$ w = w - \\alpha \\frac{\\partial J}{\\partial w} $$\n",
        "\n",
        "onde:\n",
        "- $ \\alpha $ é a taxa de aprendizado,\n",
        "- $ J $ é a função de custo,\n",
        "- $ \\frac{\\partial J}{\\partial w} $ é o gradiente da função de custo em relação ao peso $ w $."
      ],
      "metadata": {
        "id": "gUoSLDa9mrah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 4. Funções de Ativação\n",
        "\n",
        "As funções de ativação introduzem não-linearidade nas redes neurais. As mais comuns são:\n",
        "\n",
        "- **ReLU (Rectified Linear Unit)**: $ f(x) = \\max(0, x) $\n",
        "- **Sigmoid**: $ f(x) = \\frac{1}{1 + e^{-x}} $\n",
        "- **TanH (Tangente Hiperbólica)**: $ f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $\n",
        "- **Softmax**: usada na camada de saída para problemas de classificação.\n"
      ],
      "metadata": {
        "id": "GIYVs4ESms8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Implementação Prática\n",
        "\n",
        "Agora que entendemos a teoria, podemos implementar uma rede neural simples para classificar imagens do dataset MNIST. O modelo terá uma camada de entrada, uma camada oculta com ativação ReLU, e uma camada de saída com ativação Softmax."
      ],
      "metadata": {
        "id": "6mRyMm5Lmu0z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glMkQthymi2y"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Funções de ativação\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    exps = np.exp(x - np.max(x))  # Evita overflow\n",
        "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "\n",
        "# Função de perda (cross-entropy)\n",
        "def cross_entropy_loss(y_true, y_pred):\n",
        "    m = y_true.shape[0]\n",
        "    loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m  # Adiciona epsilon para evitar log(0)\n",
        "    return loss\n",
        "\n",
        "# Derivada da cross-entropy em relação à saída da softmax\n",
        "def cross_entropy_derivative(y_true, y_pred):\n",
        "    return y_pred - y_true\n",
        "\n",
        "# Inicialização dos pesos\n",
        "def initialize_weights(input_size, hidden_size, output_size):\n",
        "    W1 = np.random.randn(input_size, hidden_size) * 0.01  # Pesos da camada oculta\n",
        "    b1 = np.zeros((1, hidden_size))  # Viés da camada oculta\n",
        "    W2 = np.random.randn(hidden_size, output_size) * 0.01  # Pesos da camada de saída\n",
        "    b2 = np.zeros((1, output_size))  # Viés da camada de saída\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Propagação para frente\n",
        "def forward_propagation(X, W1, b1, W2, b2):\n",
        "    Z1 = np.dot(X, W1) + b1\n",
        "    A1 = relu(Z1)\n",
        "    Z2 = np.dot(A1, W2) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    return Z1, A1, Z2, A2\n",
        "\n",
        "# Backpropagation\n",
        "def backward_propagation(X, Y, Z1, A1, Z2, A2, W1, W2, learning_rate):\n",
        "    m = X.shape[0]\n",
        "\n",
        "    # Cálculo do gradiente da camada de saída\n",
        "    dZ2 = cross_entropy_derivative(Y, A2)\n",
        "    dW2 = np.dot(A1.T, dZ2) / m\n",
        "    db2 = np.sum(dZ2, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Cálculo do gradiente da camada oculta\n",
        "    dA1 = np.dot(dZ2, W2.T)\n",
        "    dZ1 = dA1 * relu_derivative(Z1)\n",
        "    dW1 = np.dot(X.T, dZ1) / m\n",
        "    db1 = np.sum(dZ1, axis=0, keepdims=True) / m\n",
        "\n",
        "    # Atualização dos pesos\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Transformar rótulos em one-hot encoding\n",
        "def one_hot_encoding(y, num_classes):\n",
        "    m = y.shape[0]\n",
        "    one_hot = np.zeros((m, num_classes))\n",
        "    one_hot[np.arange(m), y] = 1\n",
        "    return one_hot\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Treinamento da Rede Neural\n",
        "def train(X_train, Y_train, X_test, Y_test, input_size, hidden_size, output_size, epochs, learning_rate):\n",
        "    W1, b1, W2, b2 = initialize_weights(input_size, hidden_size, output_size)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Forward propagation\n",
        "        Z1, A1, Z2, A2 = forward_propagation(X_train, W1, b1, W2, b2)\n",
        "\n",
        "        # Cálculo da perda\n",
        "        loss = cross_entropy_loss(Y_train, A2)\n",
        "\n",
        "        # Backpropagation\n",
        "        W1, b1, W2, b2 = backward_propagation(X_train, Y_train, Z1, A1, Z2, A2, W1, W2, learning_rate)\n",
        "\n",
        "        # Avaliação a cada 10 épocas\n",
        "        if epoch % 10 == 0:\n",
        "            _, _, _, A2_test = forward_propagation(X_test, W1, b1, W2, b2)\n",
        "            test_loss = cross_entropy_loss(Y_test, A2_test)\n",
        "            accuracy = np.mean(np.argmax(A2_test, axis=1) == np.argmax(Y_test, axis=1))\n",
        "            print(f'Época {epoch}: Perda treino = {loss:.4f}, Perda teste = {test_loss:.4f}, Acurácia teste = {accuracy:.4f}')\n",
        "\n",
        "    return W1, b1, W2, b2\n",
        "\n",
        "# Predição\n",
        "def predict(X, W1, b1, W2, b2):\n",
        "    _, _, _, A2 = forward_propagation(X, W1, b1, W2, b2)\n",
        "    return np.argmax(A2, axis=1)\n"
      ],
      "metadata": {
        "id": "0gtjIDLqElsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Carregar o dataset MNIST\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# Preparação dos dados\n",
        "X_train = X_train.reshape(X_train.shape[0], -1) / 255.0  # Normaliza e achata as imagens\n",
        "X_test = X_test.reshape(X_test.shape[0], -1) / 255.0\n",
        "\n",
        "Y_train = one_hot_encoding(Y_train, 10)  # Converter para one-hot encoding\n",
        "Y_test = one_hot_encoding(Y_test, 10)\n",
        "\n",
        "# Definir hiperparâmetros\n",
        "input_size = 784  # 28x28 pixels\n",
        "hidden_size = 128\n",
        "output_size = 10   # 10 classes (0 a 9)\n",
        "epochs = 50\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Treinar a rede neural\n",
        "W1, b1, W2, b2 = train(X_train, Y_train, X_test, Y_test, input_size, hidden_size, output_size, epochs, learning_rate)\n",
        "\n",
        "# Testar com um exemplo\n",
        "index = 0\n",
        "plt.imshow(X_test[index].reshape(28, 28), cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "predicted_label = predict(X_test[index:index+1], W1, b1, W2, b2)\n",
        "print(f'Classe predita: {predicted_label[0]}')\n"
      ],
      "metadata": {
        "id": "pEpbgRhCFJA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fazendo na mão\n",
        "\n"
      ],
      "metadata": {
        "id": "6opnNf-wNYkS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Passo 1: Definir a Função de Custo**\n",
        "\n",
        "A função de custo que usamos para redes neurais de classificação binária é a **Entropia Cruzada (Binary Cross-Entropy)**:\n",
        "\n",
        "$$\n",
        "J = - \\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n",
        "$$\n",
        "\n",
        "onde:\n",
        "- $ y $ é o rótulo real da amostra (0 ou 1).\n",
        "- $ \\hat{y} = A_3 $ é a saída da rede neural (previsão).\n",
        "\n",
        "Se considerarmos que a **saída real** para este exemplo é $ y = 1 $, então a perda é:\n",
        "\n",
        "$$\n",
        "J = - \\log(0.765) \\approx 0.268\n",
        "$$\n",
        "\n",
        "Agora, usamos o **Gradiente Descendente** para atualizar os pesos.\n",
        "\n",
        "\n",
        "## **Passo 2: Cálculo do Gradiente para a Camada de Saída**\n",
        "\n",
        "A derivada da função de custo em relação à saída da rede $ A_3 $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial A_3} = - \\frac{y}{A_3} + \\frac{1 - y}{1 - A_3}\n",
        "$$\n",
        "\n",
        "Substituindo $ y = 1 $ e $ A_3 = 0.765 $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial A_3} = - \\frac{1}{0.765} + 0 = -1.307\n",
        "$$\n",
        "\n",
        "A derivada da **função sigmoide** $ A_3 = \\sigma(Z_3) $ é:\n",
        "\n",
        "$$\n",
        "\\sigma'(Z_3) = A_3 (1 - A_3)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma'(1.176) = 0.765 \\times (1 - 0.765) = 0.1798\n",
        "$$\n",
        "\n",
        "Usamos a **regra da cadeia** para calcular a derivada em relação a $ Z_3 $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial Z_3} = \\frac{\\partial J}{\\partial A_3} \\times \\frac{\\partial A_3}{\\partial Z_3}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= -1.307 \\times 0.1798 = -0.235\n",
        "$$\n",
        "\n",
        "Agora, calculamos os gradientes para os pesos $ W_3 $ e viés $ b_3 $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial W_3} = A_2^T \\times \\frac{\\partial J}{\\partial Z_3}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b_3} = \\frac{\\partial J}{\\partial Z_3}\n",
        "$$\n",
        "\n",
        "Substituindo $ A_2 = [0.367, 0, 1.275] $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial W_3} = \\begin{bmatrix} 0.367 \\\\ 0 \\\\ 1.275 \\end{bmatrix} \\times (-0.235)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\begin{bmatrix} -0.0863 \\\\ 0 \\\\ -0.2993 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b_3} = -0.235\n",
        "$$\n",
        "\n",
        "Atualizando os pesos da camada de saída:\n",
        "\n",
        "$$\n",
        "W_3 := W_3 - \\alpha \\frac{\\partial J}{\\partial W_3}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b_3 := b_3 - \\alpha \\frac{\\partial J}{\\partial b_3}\n",
        "$$\n",
        "\n",
        "Se a taxa de aprendizado for $ \\alpha = 0.1 $:\n",
        "\n",
        "$$\n",
        "W_3 = W_3 + \\begin{bmatrix} 0.00863 \\\\ 0 \\\\ 0.02993 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b_3 = b_3 + 0.0235\n",
        "$$\n",
        "\n",
        "\n",
        "## **Passo 3: Gradiente da Segunda Camada Oculta**\n",
        "\n",
        "A derivada do erro em relação à saída da segunda camada $ A_2 $:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial A_2} = W_3 \\times \\frac{\\partial J}{\\partial Z_3}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\begin{bmatrix} 0.5 \\\\ -0.3 \\\\ 0.7 \\end{bmatrix} \\times (-0.235)\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\begin{bmatrix} -0.1175 \\\\ 0.0705 \\\\ -0.1645 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "A ativação da segunda camada usa **ReLU**, então a derivada é:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial A_2}{\\partial Z_2} = \\begin{bmatrix} 1 & 0 & 1 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Multiplicando:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial Z_2} = \\frac{\\partial J}{\\partial A_2} \\times \\frac{\\partial A_2}{\\partial Z_2}\n",
        "$$\n",
        "\n",
        "$$\n",
        "= \\begin{bmatrix} -0.1175 \\\\ 0 \\\\ -0.1645 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Atualizando os pesos:\n",
        "\n",
        "$$\n",
        "W_2 = W_2 + 0.1 \\times \\begin{bmatrix} 0.1645 \\\\ 0 \\\\ 0.1202 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "$$\n",
        "b_2 = b_2 + 0.1 \\times \\begin{bmatrix} 0.1175 \\\\ 0 \\\\ 0.1645 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "## **Resumo Final**\n",
        "\n",
        "1. **Cálculo da perda** (Binary Cross-Entropy).\n",
        "2. **Erro da saída**: calculamos os gradientes da última camada.\n",
        "3. **Propagação do erro**: ajustamos os pesos das camadas intermediárias.\n",
        "4. **Atualização dos pesos** com a regra:\n",
        "\n",
        "$$\n",
        "W := W - \\alpha \\frac{\\partial J}{\\partial W}\n",
        "$$\n",
        "\n",
        "Esse processo é repetido para múltiplas amostras até que os pesos estejam otimizados.\n"
      ],
      "metadata": {
        "id": "hWNpJPGwNpS3"
      }
    }
  ]
}